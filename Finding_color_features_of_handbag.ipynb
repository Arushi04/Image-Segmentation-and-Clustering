{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each product image, I would like you to do the following. The output of this task will have the following variables for each image and append them to \n",
    "Read the image files and for each handbag image, find the below features and append it to ProductInfo.csv\n",
    "1.\tLogo size (logoSize)\n",
    "2.\tLogo contrast (logoConst)\n",
    "3.\tLogo conspicuousness (logoConsp)\n",
    "4.\tNumber of colors (numColors)\n",
    "5.\tColor Entropy (ColorEnt)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connecting to Google Cloud\n",
    "\n",
    "from google.cloud import vision\n",
    "from google.oauth2 import service_account\n",
    "\n",
    "creds = service_account.Credentials.from_service_account_file('My First Project-c1dd0c0c9fd5.json')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import io\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cv2\n",
    "import math\n",
    "import random\n",
    "import tempfile\n",
    "from PIL import Image as Img\n",
    "from IPython.display import Image\n",
    "import webcolors\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "from skimage.segmentation import slic\n",
    "from skimage.segmentation import mark_boundaries\n",
    "from skimage.util import img_as_float\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "%matplotlib inline\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_image(img, img_path):\n",
    "    '''\n",
    "    This function takes a PIL image as an input and the image in bytes, detects handbag and returns the vertices of the handbag\n",
    "    Input : PIL imaage and image in bytes\n",
    "    Output : cropped handbag and its size\n",
    "    \n",
    "    content = cv2.imencode('.png', np.array(img))[1].tobytes()\n",
    "    image = vision.Image(content=content) (converting pil to numpy detects very less logos)\n",
    "    image = vision.Image(content=img_bytes) # directly using img bytes (doesn't detect all logos)\n",
    "    '''\n",
    "\n",
    "    with io.open(img_path, 'rb') as image_file:\n",
    "        content = image_file.read()\n",
    "    image = vision.Image(content=content)\n",
    "          \n",
    "    height, width = img.size\n",
    "\n",
    "    objects = client.object_localization(image=image).localized_object_annotations\n",
    "\n",
    "    cropped_imgs = []\n",
    "    for object_ in objects:\n",
    "        if object_.name =='Bag' or object_.name == 'handbag':\n",
    "            vects = object_.bounding_poly.normalized_vertices\n",
    "            \n",
    "            x0, x2 = vects[0].x, vects[2].x\n",
    "            x0, x2 = x0*width, x2*width\n",
    "            y0, y2 = vects[0].y, vects[2].y\n",
    "            y0, y2 = y0*height, y2*height\n",
    "            \n",
    "            vects = [x0, y0, x2-1, y2-1]\n",
    "            \n",
    "            crop_img = img.crop(vects)\n",
    "            cropped_imgs.append(crop_img)\n",
    "    \n",
    "    if len(cropped_imgs) > 0:\n",
    "        cropped_imgs[0].save('cropped.jpg')\n",
    "        return cropped_imgs[0]\n",
    "    else:\n",
    "        img.save('cropped.jpg')\n",
    "        return img\n",
    "\n",
    "\n",
    "def get_logo(bag_img, img_path):\n",
    "    '''\n",
    "     This functions takes cropped handbag image in PIL format, detects logo and returns the cropped logo\n",
    "     Input: Cropped handbag image in PIL\n",
    "     Output: Cropped logo and its size if logo exists else None, 0\n",
    "     \n",
    "     #content = cv2.imencode('.png', np.array(bag_img))[1].tobytes()\n",
    "     #image = vision.Image(content=content)\n",
    "     #image = vision.Image(content=cropped_img_bytes)\n",
    "    '''\n",
    "    \n",
    "    with io.open(img_path, 'rb') as image_file:\n",
    "        content = image_file.read()\n",
    "    image = vision.Image(content=content)\n",
    "        \n",
    "    # Detects logo and returns the size of the logo and vertices  \n",
    "    response = client.logo_detection(image=image)\n",
    "    annotations = response.logo_annotations\n",
    "    #print(\"Annotations  in logo vertices func : \", annotations)\n",
    "    if len(annotations) != 0:\n",
    "        for annotation in annotations:\n",
    "            vects = annotation.bounding_poly.vertices\n",
    "            #print(vects)\n",
    "    else:\n",
    "        vects = []\n",
    "        \n",
    "    if len(vects) > 0: \n",
    "        cropped_logo = bag_img.crop([vects[0].x, vects[0].y, vects[2].x - 1, vects[2].y - 1]) \n",
    "        #Img.fromarray(np.array(cropped_logo)).show()\n",
    "        #print(\"Logo found\")\n",
    "        return cropped_logo\n",
    "    else:\n",
    "        return None\n",
    "        \n",
    "    \n",
    "\n",
    "def closest_color(requested_color):\n",
    "    '''\n",
    "     This functions takes a color in rgb and returns the color that closely resembles to that color\n",
    "     Input: Color\n",
    "     Output: color closest to the minimum color\n",
    "    '''\n",
    "\n",
    "    min_colors = {}\n",
    "    for key, name in webcolors.css3_hex_to_names.items():\n",
    "        r_c, g_c, b_c = webcolors.hex_to_rgb(key)\n",
    "        rd = (r_c - requested_color[0]) ** 2\n",
    "        gd = (g_c - requested_color[1]) ** 2\n",
    "        bd = (b_c - requested_color[2]) ** 2\n",
    "        min_colors[(rd + gd + bd)] = name\n",
    "        \n",
    "    return min_colors[min(min_colors.keys())]\n",
    "\n",
    "\n",
    "\n",
    "def get_color_name(requested_color):\n",
    "    '''\n",
    "     This functions takes a color in rgb and returns the name of the color.\n",
    "     Input: Color\n",
    "     Output: Name of the color\n",
    "    '''\n",
    "\n",
    "    try:\n",
    "        closest_name = actual_name = webcolors.rgb_to_name(requested_color)\n",
    "    except ValueError:\n",
    "        closest_name = closest_color(requested_color)\n",
    "        actual_name = None\n",
    "        \n",
    "    return actual_name, closest_name\n",
    "\n",
    "\n",
    "def centroid_histogram(clt):\n",
    "    '''\n",
    "     This functions takes the labels from the kmeans model and creates a histogram\n",
    "     Input: fitted model\n",
    "     Output: creates a histogram of all the colors based on labels\n",
    "    '''\n",
    "    # grab the number of different clusters based on the number of pixels assigned to each cluster\n",
    "\n",
    "    numLabels = np.arange(0, len(np.unique(clt.labels_)) + 1)\n",
    "    (hist, _) = np.histogram(clt.labels_, bins = numLabels)\n",
    "    \n",
    "    # normalize the histogram, such that it sums to one\n",
    "    hist = hist.astype(\"float\")\n",
    "    hist /= hist.sum()\n",
    "    \n",
    "    return hist\n",
    "\n",
    "\n",
    "\n",
    "def get_most_dominant_color(bar):\n",
    "    '''\n",
    "     This functions takes all the colors and returns the most dominant color, ratio and rgb_color\n",
    "     Input: all the colors\n",
    "     Output: Most dominant color with its ratio and rgb value\n",
    "    '''\n",
    "\n",
    "    dominant_color = \"black\"\n",
    "    max_ratio = 0\n",
    "\n",
    "    for colors in bar:\n",
    "        if colors[1] > max_ratio:\n",
    "            max_ratio = colors[1]\n",
    "            dominant_color = colors[0]\n",
    "            rgb_color = colors[2]\n",
    "        \n",
    "    return dominant_color, max_ratio, rgb_color  \n",
    "\n",
    "\n",
    "\n",
    "def get_sorted_colors(all_colors):\n",
    "    '''\n",
    "     This functions takes all the colors which can be duplicate and returns the list of unique colors in\n",
    "     decending order based on their total ratio\n",
    "     Input: all the colors\n",
    "     Output: list of unoque colors in decending order\n",
    "    '''\n",
    "    \n",
    "    color_pixels = {}\n",
    "\n",
    "    for color in all_colors:\n",
    "        if color[0] not in color_pixels:\n",
    "            color_pixels[color[0]] = color[1]\n",
    "        else:\n",
    "            color_pixels[color[0]] += color[1]\n",
    "\n",
    "    sorted_colors_pixels = sorted(color_pixels.items(), key=lambda x: x[1], reverse=True) \n",
    "    \n",
    "    return sorted_colors_pixels\n",
    "\n",
    "\n",
    "\n",
    "def get_colors(hist, centroids):\n",
    "    '''\n",
    "     This functions takes the histogram and labels and returns dominant color, it's ratio, rgb value and all colors\n",
    "     Input: histogram and labels\n",
    "     Output: dominant_color, ratio, rgb_color, all_colors\n",
    "    '''\n",
    "    \n",
    "    # initialize the bar chart representing the relative frequency of each of the colors\n",
    "    bar = np.zeros((50, 300, 3), dtype = \"uint8\")\n",
    "    startX = 0\n",
    "    all_colors = []\n",
    "    \n",
    "    # loop over the percentage and color of each cluster\n",
    "    for (percent, color) in zip(hist, centroids):\n",
    "        requested_color = color.astype(int)\n",
    "        \n",
    "        actual_name, closest_name = get_color_name(requested_color)\n",
    "        \n",
    "        if actual_name is None:\n",
    "            all_colors.append([closest_name, percent, requested_color])\n",
    "        else:\n",
    "            all_colors.append([actual_name, percent, requested_color])\n",
    "            \n",
    "        endX = startX + (percent * 300)\n",
    "        cv2.rectangle(bar, (int(startX), 0), (int(endX), 50),\n",
    "        color.astype(\"uint8\").tolist(), -1)\n",
    "        startX = endX\n",
    "         \n",
    "    dominant_color, ratio, rgb_color = get_most_dominant_color(all_colors)\n",
    "\n",
    "    return bar, dominant_color, ratio, rgb_color, all_colors\n",
    "\n",
    "\n",
    "\n",
    "def mean_image(image, segments):\n",
    "    '''\n",
    "    This function takes segmented image and no of segments and returns the image with average of superpixels\n",
    "    Input: Segmented Image, No of segments\n",
    "    Output: Image with average of colors in superpixels\n",
    "    '''\n",
    "\n",
    "    reshaped_image = image.reshape((image.shape[0]*image.shape[1], image.shape[2]))\n",
    "    segment_1d = np.reshape(segments, -1)    \n",
    "    unique_segment = np.unique(segment_1d)\n",
    "    img_shape = np.zeros(reshaped_image.shape)\n",
    "    \n",
    "    for i in unique_segment:\n",
    "        loc = np.where(segment_1d==i)[0]\n",
    "        img_mean = np.mean(reshaped_image[loc,:], axis=0)\n",
    "        img_shape[loc,:] = img_mean\n",
    "        \n",
    "    out_img = np.reshape(img_shape,[image.shape[0], image.shape[1], image.shape[2]]).astype('uint8')\n",
    "    \n",
    "    return out_img \n",
    "    \n",
    "\n",
    "\n",
    "def segment_image(cropped_img, numSegments):\n",
    "    '''\n",
    "    Segements the cropped image as input and no of segments and convert it into superpixels\n",
    "    Input: Cropped Image, Number of segments\n",
    "    Output: Segmented image and image with average of colors in superpixels\n",
    "    '''\n",
    "\n",
    "    converted_img = img_as_float(cropped_img[:,:,::-1]) #convert it to a floating point data type\n",
    "    segments = slic(converted_img, n_segments=numSegments, compactness=10, sigma=5, convert2lab=True)\n",
    "    \n",
    "    # Average the color in each superpixel.\n",
    "    out_img = mean_image(cropped_img, segments)\n",
    "    \n",
    "    return out_img\n",
    "\n",
    "\n",
    "\n",
    "def segment_and_cluster(num_segments, cropped_img, cluster_size):\n",
    "    '''\n",
    "    This function segments the image, cluster the pixels using kmeans clustering and return the complete details of colors\n",
    "    Input: number of segments to be done, cropped image, cluster size\n",
    "    Output: bar, dominant_color, ratio, rgb_color, all_colors\n",
    "    '''\n",
    "    \n",
    "    # Segmentation of the image into superpixels and taking average of superpixels\n",
    "    out_img = segment_image(cropped_img, num_segments)\n",
    "    \n",
    "    # cluster the pixel intensities   \n",
    "\n",
    "    clt = KMeans(n_clusters = cluster_size, random_state=42)\n",
    "    clt.fit(out_img.reshape(-1,3))\n",
    "\n",
    "    hist = centroid_histogram(clt)\n",
    "    bar, dominant_color, ratio, rgb_color, all_colors = get_colors(hist, clt.cluster_centers_)\n",
    "    \n",
    "    return bar, dominant_color, ratio, rgb_color, all_colors\n",
    "   \n",
    "    \n",
    "    \n",
    "def calculate_luminace(color_code):\n",
    "    '''\n",
    "    This function calculates the luminance of color\n",
    "    Input: a color code in rgb\n",
    "    Output: luminance\n",
    "    '''\n",
    "    \n",
    "    index = float(color_code)/255 \n",
    "\n",
    "    if index < 0.03928:\n",
    "        return index / 12.92\n",
    "    else:\n",
    "        return ((index + 0.055) / 1.055)**2.4\n",
    "    \n",
    "\n",
    "    \n",
    "def calculate_relative_luminance(rgb):\n",
    "    '''\n",
    "    This function calculates the relative luminance of the color using the 3 channels of R, G, B\n",
    "    Input: a color in rgb\n",
    "    Output: relative luminance\n",
    "    '''\n",
    "    \n",
    "    return 0.2126*calculate_luminace(rgb[0]) + 0.7152*calculate_luminace(rgb[1]) + 0.0722*calculate_luminace(rgb[2])\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total sellers :  114\n",
      "Logo size :  15805\n",
      "Contrast ratio is :  1.128333572789869\n",
      "Logo Conspicuousness :  0.020350646372989993\n",
      "Total colors are :  7\n",
      "Entropy of handbag is :  0.10184623665219385\n",
      "Total colors are :  7\n",
      "Entropy of handbag is :  0.03439641234078021\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-2796e806ffc9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[0mclient\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvision\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mImageAnnotatorClient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcredentials\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m \u001b[0mproduct_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread_images\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Group4/Sellers/'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclient\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#seller_105548/Images')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m \u001b[0mproduct_info\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-19-2796e806ffc9>\u001b[0m in \u001b[0;36mread_images\u001b[0;34m(dir_path, client)\u001b[0m\n\u001b[1;32m    120\u001b[0m         \u001b[0mitem_details\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m     \u001b[0mitem_details\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnew_cols\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mitem_details\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Image_path'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocess_images\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/pytorch/lib/python3.7/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, func, convert_dtype, args, **kwds)\u001b[0m\n\u001b[1;32m   4198\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4199\u001b[0m                 \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4200\u001b[0;31m                 \u001b[0mmapped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_infer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconvert_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4201\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4202\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSeries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/lib.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m<ipython-input-19-2796e806ffc9>\u001b[0m in \u001b[0;36mprocess_images\u001b[0;34m(img_path)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0;31m#print(\"Processing for bag\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m     \u001b[0mbar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbag_dominant_color\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbag_ratio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbag_rgb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_bag_colors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msegment_and_cluster\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_segments\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcropped_bag\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcluster_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m     \u001b[0msorted_bag_colors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_sorted_colors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_bag_colors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0;31m#print(\"Sorted bag colors : \", sorted_bag_colors)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-17-98d1d12fafe2>\u001b[0m in \u001b[0;36msegment_and_cluster\u001b[0;34m(num_segments, cropped_img, cluster_size)\u001b[0m\n\u001b[1;32m    255\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m     \u001b[0;31m# Segmentation of the image into superpixels and taking average of superpixels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 257\u001b[0;31m     \u001b[0mout_img\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msegment_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcropped_img\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_segments\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m     \u001b[0;31m# cluster the pixel intensities\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-17-98d1d12fafe2>\u001b[0m in \u001b[0;36msegment_image\u001b[0;34m(cropped_img, numSegments)\u001b[0m\n\u001b[1;32m    238\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    239\u001b[0m     \u001b[0mconverted_img\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg_as_float\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcropped_img\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#convert it to a floating point data type\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 240\u001b[0;31m     \u001b[0msegments\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mslic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconverted_img\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_segments\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnumSegments\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompactness\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msigma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert2lab\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    241\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    242\u001b[0m     \u001b[0;31m# Average the color in each superpixel.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/pytorch/lib/python3.7/site-packages/skimage/segmentation/slic_superpixels.py\u001b[0m in \u001b[0;36mslic\u001b[0;34m(image, n_segments, compactness, max_iter, sigma, spacing, multichannel, convert2lab, enforce_connectivity, min_size_factor, max_size_factor, slic_zero)\u001b[0m\n\u001b[1;32m    170\u001b[0m     \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mascontiguousarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mratio\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 172\u001b[0;31m     \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_slic_cython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msegments\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mspacing\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mslic_zero\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0menforce_connectivity\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mskimage/segmentation/_slic.pyx\u001b[0m in \u001b[0;36mskimage.segmentation._slic._slic_cython\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/pytorch/lib/python3.7/site-packages/numpy/core/_asarray.py\u001b[0m in \u001b[0;36masarray\u001b[0;34m(a, dtype, order)\u001b[0m\n\u001b[1;32m     12\u001b[0m ]\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0;34m@\u001b[0m\u001b[0mset_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'numpy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \"\"\"Convert the input to an array.\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def process_images(img_path):\n",
    "    \n",
    "    if not os.path.isfile(img_path):\n",
    "        return pd.Series([None]*5)\n",
    "    \n",
    "    # Get the cropped bag\n",
    "    img = Img.open(img_path)\n",
    "    cropped_bag = crop_image(img, img_path) \n",
    "    height, width = cropped_bag.size\n",
    "    bag_size = height*width\n",
    "    #print(\"Bag size : \", bag_size)\n",
    "    #Img.fromarray(np.array(cropped_bag)).show()\n",
    "    \n",
    "    # Get the cropped logo\n",
    "    cropped_img_path = 'cropped.jpg'\n",
    "    cropped_img = Img.open(cropped_img_path)\n",
    "    cropped_logo= get_logo(cropped_img, cropped_img_path) \n",
    "    \n",
    "\n",
    "    # converting cropped bag img from PIL to numpy array\n",
    "    cropped_bag = np.array(cropped_bag)\n",
    "    \n",
    "\n",
    "    # Segmentation of the image into superpixels, taking average of superpixels and then doing clustering\n",
    "    num_segments = 600\n",
    "    cluster_size = 10\n",
    "\n",
    "    #print(\"Processing for bag\")\n",
    "    bar, bag_dominant_color, bag_ratio, bag_rgb, all_bag_colors = segment_and_cluster(num_segments, cropped_bag, cluster_size)\n",
    "    sorted_bag_colors = get_sorted_colors(all_bag_colors)\n",
    "    #print(\"Sorted bag colors : \", sorted_bag_colors)\n",
    "\n",
    "\n",
    "    #print(\"Processing for logo\")\n",
    "    \n",
    "    if cropped_logo is not None:\n",
    "        cropped_logo = np.array(cropped_logo)  # converting pil image to numpy array\n",
    "        num_segments = 300\n",
    "        cluster_size = 3\n",
    "\n",
    "        # Get logo size \n",
    "        height, width, _ = cropped_logo.shape\n",
    "        logo_size = height * width\n",
    "        print(\"Logo size : \", logo_size)\n",
    "\n",
    "        # Segment and cluster logo colors\n",
    "        bar, logo_dominant_color, logo_ratio, logo_rgb, all_logo_colors = segment_and_cluster(num_segments, cropped_logo, cluster_size)\n",
    "        sorted_logo_colors = get_sorted_colors(all_logo_colors)\n",
    "        #print(\"logo colors : \", sorted_logo_colors)\n",
    "\n",
    "        # Calculate contrast ratio of logo to bag\n",
    "        contrast_ratio = (calculate_relative_luminance(logo_rgb)+0.05) / (calculate_relative_luminance(bag_rgb)+0.05)\n",
    "        print(\"Contrast ratio is : \", contrast_ratio)\n",
    "        \n",
    "        # Calculate Logo Conspicuousness\n",
    "        relative_size = logo_size/bag_size\n",
    "        logo_conspicuousness = contrast_ratio * relative_size\n",
    "        print(\"Logo Conspicuousness : \", logo_conspicuousness)\n",
    "\n",
    "    else:\n",
    "        logo_size = 0.0\n",
    "        contrast_ratio = 0.0\n",
    "        logo_conspicuousness = 0.0\n",
    "\n",
    "\n",
    "    # Find total number of colors in the bag\n",
    "    total_colors = len(sorted_bag_colors)\n",
    "    print(\"Total colors are : \", total_colors)\n",
    "\n",
    "\n",
    "    # Calculate Entropy of the bag\n",
    "    total_sum = 0\n",
    "\n",
    "    for color in sorted_bag_colors:\n",
    "        if color[1] >= 0.05:\n",
    "            total_sum += color[1]\n",
    "\n",
    "    entropy = -1 * (total_sum * math.log(total_sum))\n",
    "    print(\"Entropy of handbag is : \", entropy)\n",
    "\n",
    "    \n",
    "    return pd.Series((logo_size, round(contrast_ratio, 3), round(logo_conspicuousness, 3), total_colors, round(entropy, 3)))\n",
    "\n",
    "\n",
    "\n",
    "def read_images(dir_path, client):\n",
    "    count = 0\n",
    "    \n",
    "    item_details = pd.DataFrame(columns = ['Seller', 'Item ID', 'category', 'brand', 'Image_path']) \n",
    "    dir_names = os.listdir(dir_path)\n",
    "    print(\"Total sellers : \", len(dir_names))\n",
    "    for seller in dir_names:\n",
    "        if not seller.startswith('seller_'):\n",
    "            continue\n",
    "        \n",
    "        product_info = pd.read_csv(dir_path+seller+'/ProductInfo.csv', usecols=[\"Item ID\", \"category\", \"brand\"])\n",
    "        product_info['Seller'] = seller\n",
    "\n",
    "        \n",
    "        # Get the image names and add to the dataframe\n",
    "        product_info['Image_path'] = ''\n",
    "        img_fnames = os.listdir(dir_path + seller + '/Images')\n",
    "        for idx in range(len(img_fnames)):\n",
    "            img_path = dir_path + seller + '/Images/' + img_fnames[idx]\n",
    "            img_id = int(img_fnames[idx].split('-')[0])\n",
    "            img_fnames[idx] =  [img_path, img_id]\n",
    "        \n",
    "        img_ids, img_names = map(list, zip(*img_fnames))\n",
    "        product_info['Image_path'] = product_info['Item ID'].replace(to_replace=img_names, value=img_ids)\n",
    "        \n",
    "        item_details = pd.concat([item_details, product_info], ignore_index=True)\n",
    "        #import ipdb; ipdb.set_trace()\n",
    "    \n",
    "    # Removing the rows that doesn't contain bags\n",
    "    item_details = item_details[item_details['category'].str.match('Bags')]\n",
    "    \n",
    "    \n",
    "    new_cols = ['Logo Size', 'Logo Contrast', 'Logo Conspicuousness', 'Number of colors', 'Color Entropy']\n",
    "    for col in new_cols:\n",
    "        item_details[col] = None\n",
    "    \n",
    "    item_details[new_cols] = item_details['Image_path'].apply(process_images)\n",
    "    \n",
    "    return test\n",
    "    #return item_details\n",
    "\n",
    "client = vision.ImageAnnotatorClient(credentials=creds)\n",
    "product_info = read_images('Group4/Sellers/', client) #seller_105548/Images')\n",
    "product_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "product_info.to_csv(\"bag_details.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find and crop handbag and return the objects's vertices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Group4/Sellers/seller_103176/Images/7187613-1_3.jpg\n",
      "Logo found\n",
      "Group4/Sellers/seller_10225/Images/3533981-1_1.jpg\n",
      "Group4/Sellers/seller_10225/Images/3457893-1_1.jpg\n",
      "Group4/Sellers/seller_10225/Images/3458071-1_1.jpg\n",
      "Group4/Sellers/seller_10225/Images/2606327-1_1.jpg\n",
      "Group4/Sellers/seller_10225/Images/2606305-1_2.jpg\n",
      "Logo found\n",
      "Group4/Sellers/seller_1039119/Images/6176374-1_1.jpg\n",
      "Group4/Sellers/seller_1070585/Images/1235369-1_1.jpg\n",
      "Group4/Sellers/seller_1007672/Images/9833455-1_1.jpg\n",
      "Group4/Sellers/seller_1027448/Images/3761728-1_1.jpg\n",
      "Group4/Sellers/seller_1027448/Images/3240142-1_4.jpg\n",
      "Group4/Sellers/seller_1027448/Images/3199111-1_1.jpg\n",
      "Group4/Sellers/seller_10016968/Images/11646796-1_1.jpg\n",
      "Group4/Sellers/seller_10016968/Images/11384055-1.jpg\n",
      "Group4/Sellers/seller_10016968/Images/10955231-1_1.jpg\n",
      "Group4/Sellers/seller_10016968/Images/10907197-1_1.jpg\n",
      "Group4/Sellers/seller_10016968/Images/10906053-1_1.jpg\n",
      "Group4/Sellers/seller_1009292/Images/1003697-1_1.jpg\n",
      "Group4/Sellers/seller_1007937/Images/8855564-1_1.jpg\n",
      "Logo found\n",
      "Group4/Sellers/seller_1023339/Images/11749274-1.jpg\n",
      "Group4/Sellers/seller_108488/Images/8213537-1_1.jpg\n",
      "Group4/Sellers/seller_1009803/Images/3480003-1_1.jpg\n",
      "Group4/Sellers/seller_1018588/Images/5832099-1_4.jpg\n",
      "Group4/Sellers/seller_1001865/Images/5359799-1_1.jpg\n",
      "Logo found\n",
      "Group4/Sellers/seller_1001865/Images/5359915-1_1.jpg\n",
      "Group4/Sellers/seller_1000386/Images/2159612-1_1.jpg\n",
      "Logo found\n",
      "Group4/Sellers/seller_1000386/Images/2159535-1_1.jpg\n",
      "Group4/Sellers/seller_1000386/Images/2159497-1_1.jpg\n",
      "Group4/Sellers/seller_1000386/Images/1925965-1_1.jpg\n",
      "Group4/Sellers/seller_102386/Images/135784-1.jpg\n",
      "Group4/Sellers/seller_1073000/Images/9050355-1_1.jpg\n",
      "Logo found\n",
      "Group4/Sellers/seller_1065368/Images/1157122-1_1.jpg\n",
      "Group4/Sellers/seller_1065368/Images/1027574-1_1.jpg\n",
      "Group4/Sellers/seller_1065368/Images/1027603-1_1.jpg\n",
      "Group4/Sellers/seller_107213/Images/903776-1_1.jpg\n",
      "Group4/Sellers/seller_10006328/Images/10886459-1_1.jpg\n",
      "Logo found\n",
      "Group4/Sellers/seller_10006328/Images/10872489-6_8.jpg\n",
      "Group4/Sellers/seller_1057713/Images/7324444-1_1.jpg\n",
      "Group4/Sellers/seller_1061282/Images/9025409-1_1.jpg\n",
      "Group4/Sellers/seller_1061282/Images/9022432-1_1.jpg\n",
      "Group4/Sellers/seller_10839/Images/937347-1_1.jpg\n",
      "Logo found\n",
      "Group4/Sellers/seller_1026978/Images/10565453-1_1.jpg\n",
      "Logo found\n",
      "Group4/Sellers/seller_1076268/Images/2621282-1_1.jpg\n",
      "Group4/Sellers/seller_100784/Images/230222-1_1.jpg\n",
      "Logo found\n",
      "Group4/Sellers/seller_102215/Images/3909075-1_1.jpg\n",
      "Logo found\n",
      "Group4/Sellers/seller_102215/Images/173011-1_1.jpg\n",
      "Group4/Sellers/seller_102215/Images/172961-1_1.jpg\n",
      "Group4/Sellers/seller_1061848/Images/11117362-1_1.jpg\n",
      "Group4/Sellers/seller_1061848/Images/11101435-1_2.jpg\n",
      "Group4/Sellers/seller_1061848/Images/10938738-1_1.jpg\n",
      "Group4/Sellers/seller_1061848/Images/10938436-1_1.jpg\n",
      "Group4/Sellers/seller_1061848/Images/10974687-1_1.jpg\n",
      "Group4/Sellers/seller_100722/Images/952168-1_1.jpg\n",
      "Group4/Sellers/seller_10024339/Images/10913280-1_1.jpg\n",
      "Group4/Sellers/seller_1088557/Images/4120451-1_1.jpg\n",
      "Group4/Sellers/seller_1088557/Images/2068579-1_1.jpg\n",
      "Group4/Sellers/seller_1088557/Images/1152396-1_2.jpg\n",
      "Logo found\n",
      "Group4/Sellers/seller_1088557/Images/1136202-1_1.jpg\n",
      "Group4/Sellers/seller_1053165/Images/3905932-940998-1.jpg\n",
      "Group4/Sellers/seller_1053165/Images/940998-1_1.jpg\n",
      "Group4/Sellers/seller_1064145/Images/1456076-1_2.jpg\n",
      "Logo found\n",
      "Group4/Sellers/seller_1064145/Images/1456063-1_1.jpg\n",
      "Group4/Sellers/seller_1064145/Images/1182237-1_1.jpg\n",
      "Group4/Sellers/seller_1070602/Images/994864-1_1.jpg\n",
      "Group4/Sellers/seller_108084/Images/419098-1_1.jpg\n",
      "Group4/Sellers/seller_1011660/Images/6785828-1_1.jpg\n",
      "Logo found\n",
      "Group4/Sellers/seller_10005343/Images/11556984-1_1.jpg\n",
      "Group4/Sellers/seller_10005343/Images/11524099-1_2.jpg\n",
      "Logo found\n",
      "Group4/Sellers/seller_10005343/Images/11523114-1_1.jpg\n",
      "Logo found\n",
      "Group4/Sellers/seller_10005343/Images/11522660-1_1.jpg\n",
      "Group4/Sellers/seller_10005343/Images/11523336-1_1.jpg\n",
      "Group4/Sellers/seller_10005343/Images/10900556-1_1.jpg\n",
      "Group4/Sellers/seller_10005343/Images/10900485-1_1.jpg\n",
      "Group4/Sellers/seller_10005343/Images/11523669-1_1.jpg\n",
      "Group4/Sellers/seller_1020189/Images/6605078-1_1.jpg\n",
      "Logo found\n",
      "Group4/Sellers/seller_1023198/Images/2282764-1_1.jpg\n",
      "Logo found\n",
      "Group4/Sellers/seller_1023198/Images/1531921-1_1.jpg\n",
      "Group4/Sellers/seller_1029697/Images/11382381-1_1.jpg\n",
      "Group4/Sellers/seller_105548/Images/7136909-1_1.jpg\n",
      "Logo found\n",
      "Group4/Sellers/seller_105548/Images/963588-1_1.jpg\n",
      "Group4/Sellers/seller_105548/Images/594715-1_1.jpg\n",
      "Group4/Sellers/seller_105548/Images/494463-1_1.jpg\n",
      "Group4/Sellers/seller_105548/Images/203551-1_1.jpg\n",
      "Group4/Sellers/seller_105548/Images/203569-1_2.jpg\n",
      "Group4/Sellers/seller_1054012/Images/4489149-4191097-1.jpg\n",
      "Group4/Sellers/seller_1087886/Images/8441403-1_1.jpg\n",
      "Group4/Sellers/seller_1016663/Images/6991829-1_1.jpg\n",
      "Group4/Sellers/seller_1002631/Images/5618989-4721366-1.jpg\n",
      "Logo found\n",
      "Group4/Sellers/seller_1002631/Images/4721366-1_1.jpg\n",
      "Logo found\n",
      "Group4/Sellers/seller_1002631/Images/4727841-1_1.jpg\n",
      "Logo found\n",
      "Group4/Sellers/seller_1060569/Images/4160613-1_1.jpg\n",
      "Group4/Sellers/seller_10010733/Images/11405898-6_2.jpg\n",
      "Logo found\n",
      "Group4/Sellers/seller_10010733/Images/11086769-1_2.jpg\n",
      "Group4/Sellers/seller_1044104/Images/3639789-1_1.jpg\n",
      "Group4/Sellers/seller_1044104/Images/3152540-2852406-1.jpg\n",
      "Group4/Sellers/seller_1085469/Images/3341984-1_1.jpg\n",
      "Group4/Sellers/seller_1011102/Images/7603156-1_1.jpg\n",
      "Group4/Sellers/seller_1011102/Images/4723283-1_1.jpg\n",
      "Logo found\n",
      "Group4/Sellers/seller_1078199/Images/3195642-3118907-1.jpg\n",
      "Group4/Sellers/seller_109053/Images/3041995-1_1.jpg\n",
      "Group4/Sellers/seller_109053/Images/3042066-1_1.jpg\n"
     ]
    }
   ],
   "source": [
    "def crop_image(img, img_name, img_path):\n",
    "    '''\n",
    "    This function takes a PIL image as an input and the image in bytes, detects handbag and returns the vertices of the handbag\n",
    "    Input : PIL imaage and image in bytes\n",
    "    Output : cropped handbag and its size\n",
    "    '''\n",
    "\n",
    "    #content = cv2.imencode('.png', np.array(img))[1].tobytes()\n",
    "    #image = vision.Image(content=content)\n",
    "    with io.open(img_path, 'rb') as image_file:\n",
    "        content = image_file.read()\n",
    "    image = vision.Image(content=content)\n",
    "        \n",
    "    #image = vision.Image(content=img_bytes)\n",
    "    height, width = img.size\n",
    "\n",
    "    objects = client.object_localization(image=image).localized_object_annotations\n",
    "\n",
    "    cropped_imgs = []\n",
    "    for object_ in objects:\n",
    "        if object_.name =='Bag' or object_.name == 'handbag':\n",
    "            vects = object_.bounding_poly.normalized_vertices\n",
    "            \n",
    "            x0, x2 = vects[0].x, vects[2].x\n",
    "            x0, x2 = x0*width, x2*width\n",
    "            y0, y2 = vects[0].y, vects[2].y\n",
    "            y0, y2 = y0*height, y2*height\n",
    "            \n",
    "            vects = [x0, y0, x2-1, y2-1]\n",
    "            \n",
    "            crop_img = img.crop(vects)\n",
    "            cropped_imgs.append(crop_img)\n",
    "    #print(cropped_imgs)\n",
    "    \n",
    "    if len(cropped_imgs) > 0:\n",
    "        cropped_imgs[0].save('cropped.jpg')\n",
    "        return cropped_imgs[0]\n",
    "    else:\n",
    "        img.save('cropped.jpg')\n",
    "        return img\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_logo(bag_img, img_path):\n",
    "    '''\n",
    "     This functions takes cropped handbag image in PIL format, detects logo and returns the cropped logo\n",
    "     Input: Cropped handbag image in PIL\n",
    "     Output: Cropped logo and its size if logo exists else None, 0\n",
    "    '''\n",
    "\n",
    "    #content = cv2.imencode('.png', np.array(bag_img))[1].tobytes()\n",
    "    #image = vision.Image(content=content)\n",
    "    #image = vision.Image(content=cropped_img_bytes)\n",
    "    \n",
    "    with io.open(img_path, 'rb') as image_file:\n",
    "        content = image_file.read()\n",
    "    image = vision.Image(content=content)\n",
    "        \n",
    "    # Detects logo and returns the size of the logo and vertices \n",
    "    #image = vision.Image(content=image)  \n",
    "    response = client.logo_detection(image=image)\n",
    "    annotations = response.logo_annotations\n",
    "    #print(\"Annotations  in logo vertices func : \", annotations)\n",
    "    if len(annotations) != 0:\n",
    "        for annotation in annotations:\n",
    "            vects = annotation.bounding_poly.vertices\n",
    "            #print(vects)\n",
    "    else:\n",
    "        vects = []\n",
    "        \n",
    "    if len(vects) > 0: \n",
    "        cropped_logo = bag_img.crop([vects[0].x, vects[0].y, vects[2].x - 1, vects[2].y - 1]) \n",
    "        Img.fromarray(np.array(cropped_logo)).show()\n",
    "        print(\"Logo found\")\n",
    "        return cropped_logo\n",
    "    else:\n",
    "        return None\n",
    "        \n",
    "\n",
    "\n",
    "def PIL2bytes(img):\n",
    "    '''\n",
    "     This functions takes a PIL image and converts the image to byte array\n",
    "     Input: PIL image\n",
    "     Output: byte array of the image\n",
    "    '''\n",
    "    \n",
    "    img_byte_arr = io.BytesIO()\n",
    "    img.save(img_byte_arr, format='png')\n",
    "    img_byte_arr = img_byte_arr.getvalue()\n",
    "    \n",
    "    return img_byte_arr\n",
    "\n",
    "\n",
    "for img_path in product_info['Image_path']:\n",
    "    if not str(img_path).startswith(\"Group\"):\n",
    "        continue\n",
    "    print(img_path)\n",
    "    img = Img.open(img_path)\n",
    "    img_name = img_path.split('/')[-1].split('.')[0]\n",
    "    #img_bytes = PIL2bytes(img)\n",
    "    cropped_bag = crop_image(img, img_name, img_path) #get cropped bag\n",
    "    #Img.fromarray(np.array(cropped_bag)).show()\n",
    "    \n",
    "    cropped_img_path = 'cropped.jpg'\n",
    "    cropped_img = Img.open(cropped_img_path)\n",
    "    \n",
    "    #cropped_img_bytes = PIL2bytes(cropped_bag)\n",
    "    cropped_logo= get_logo(cropped_img, cropped_img_path) # get cropped logo\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'product_info' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-5081be9ec5e9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mproduct_info\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'product_info' is not defined"
     ]
    }
   ],
   "source": [
    "product_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
