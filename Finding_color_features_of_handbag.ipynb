{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each product image, I would like you to do the following. The output of this task will have the following variables for each image and append them to \n",
    "Read the image files and for each handbag image, find the below features and append it to ProductInfo.csv\n",
    "1.\tLogo size (logoSize)\n",
    "2.\tLogo contrast (logoConst)\n",
    "3.\tLogo conspicuousness (logoConsp)\n",
    "4.\tNumber of colors (numColors)\n",
    "5.\tColor Entropy (ColorEnt)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connecting to Google Cloud\n",
    "\n",
    "from google.cloud import vision\n",
    "from google.oauth2 import service_account\n",
    "\n",
    "creds = service_account.Credentials.from_service_account_file('My First Project-c1dd0c0c9fd5.json')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import io\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cv2\n",
    "import math\n",
    "from math import log, e\n",
    "import random\n",
    "from PIL import Image as Img\n",
    "from IPython.display import Image\n",
    "import webcolors\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "from skimage.segmentation import slic\n",
    "from skimage.segmentation import mark_boundaries\n",
    "from skimage.util import img_as_float\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "%matplotlib inline\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_image(img, img_path):\n",
    "    '''\n",
    "    This function takes a PIL image as an input and the image in bytes, detects handbag and returns the vertices of the handbag\n",
    "    Input : PIL imaage and image in bytes\n",
    "    Output : cropped handbag and its size\n",
    "    \n",
    "    content = cv2.imencode('.png', np.array(img))[1].tobytes()\n",
    "    image = vision.Image(content=content) (converting pil to numpy detects very less logos)\n",
    "    image = vision.Image(content=img_bytes) # directly using img bytes (doesn't detect all logos)\n",
    "    '''\n",
    "\n",
    "    with io.open(img_path, 'rb') as image_file:\n",
    "        content = image_file.read()\n",
    "    image = vision.Image(content=content)\n",
    "          \n",
    "    height, width = img.size\n",
    "\n",
    "    objects = client.object_localization(image=image).localized_object_annotations\n",
    "\n",
    "    cropped_imgs = []\n",
    "    for object_ in objects:\n",
    "        if object_.name =='Bag' or object_.name == 'handbag':\n",
    "            vects = object_.bounding_poly.normalized_vertices\n",
    "            \n",
    "            x0, x2 = vects[0].x, vects[2].x\n",
    "            x0, x2 = x0*width, x2*width\n",
    "            y0, y2 = vects[0].y, vects[2].y\n",
    "            y0, y2 = y0*height, y2*height\n",
    "            \n",
    "            vects = [x0, y0, x2-1, y2-1]\n",
    "            \n",
    "            crop_img = img.crop(vects)\n",
    "            cropped_imgs.append(crop_img)\n",
    "    \n",
    "    if len(cropped_imgs) > 0:\n",
    "        cropped_imgs[0].save('cropped.jpg')\n",
    "        return cropped_imgs[0]\n",
    "    else:\n",
    "        img.save('cropped.jpg')\n",
    "        return img\n",
    "\n",
    "\n",
    "def get_logo(bag_img, img_path):\n",
    "    '''\n",
    "     This functions takes cropped handbag image in PIL format, detects logo and returns the cropped logo\n",
    "     Input: Cropped handbag image in PIL\n",
    "     Output: Cropped logo and its size if logo exists else None, 0\n",
    "     \n",
    "     #content = cv2.imencode('.png', np.array(bag_img))[1].tobytes()\n",
    "     #image = vision.Image(content=content)\n",
    "     #image = vision.Image(content=cropped_img_bytes)\n",
    "    '''\n",
    "    \n",
    "    with io.open(img_path, 'rb') as image_file:\n",
    "        content = image_file.read()\n",
    "    image = vision.Image(content=content)\n",
    "        \n",
    "    # Detects logo and returns the size of the logo and vertices  \n",
    "    response = client.logo_detection(image=image)\n",
    "    annotations = response.logo_annotations\n",
    "    #print(\"Annotations  in logo vertices func : \", annotations)\n",
    "    if len(annotations) != 0:\n",
    "        for annotation in annotations:\n",
    "            vects = annotation.bounding_poly.vertices\n",
    "            #print(vects)\n",
    "    else:\n",
    "        vects = []\n",
    "        \n",
    "    if len(vects) > 0: \n",
    "        cropped_logo = bag_img.crop([vects[0].x, vects[0].y, vects[2].x - 1, vects[2].y - 1]) \n",
    "        #Img.fromarray(np.array(cropped_logo)).show()\n",
    "        #print(\"Logo found\")\n",
    "        return cropped_logo\n",
    "    else:\n",
    "        return None\n",
    "        \n",
    "    \n",
    "\n",
    "def closest_color(requested_color):\n",
    "    '''\n",
    "     This functions takes a color in rgb and returns the color that closely resembles to that color\n",
    "     Input: Color\n",
    "     Output: color closest to the minimum color\n",
    "    '''\n",
    "\n",
    "    min_colors = {}\n",
    "    for key, name in webcolors.css21_hex_to_names.items():\n",
    "        r_c, g_c, b_c = webcolors.hex_to_rgb(key)\n",
    "        rd = (r_c - requested_color[0]) ** 2\n",
    "        gd = (g_c - requested_color[1]) ** 2\n",
    "        bd = (b_c - requested_color[2]) ** 2\n",
    "        min_colors[(rd + gd + bd)] = name\n",
    "        \n",
    "    return min_colors[min(min_colors.keys())]\n",
    "\n",
    "\n",
    "\n",
    "def get_color_name(requested_color):\n",
    "    '''\n",
    "     This functions takes a color in rgb and returns the name of the color.\n",
    "     Input: Color\n",
    "     Output: Name of the color\n",
    "    '''\n",
    "\n",
    "    try:\n",
    "        actual_name = webcolors.rgb_to_name(requested_color)\n",
    "    except ValueError:\n",
    "        closest_name = closest_color(requested_color)\n",
    "        actual_name = None\n",
    "        \n",
    "    return actual_name, closest_name\n",
    "\n",
    "\n",
    "def centroid_histogram(clt):\n",
    "    '''\n",
    "     This functions takes the labels from the kmeans model and creates a histogram\n",
    "     Input: fitted model\n",
    "     Output: creates a histogram of all the colors based on labels\n",
    "    '''\n",
    "    # grab the number of different clusters based on the number of pixels assigned to each cluster\n",
    "\n",
    "    numLabels = np.arange(0, len(np.unique(clt.labels_)) + 1)\n",
    "    (hist, _) = np.histogram(clt.labels_, bins = numLabels)\n",
    "    \n",
    "    # normalize the histogram, such that it sums to one\n",
    "    hist = hist.astype(\"float\")\n",
    "    hist /= hist.sum()\n",
    "    \n",
    "    return hist\n",
    "\n",
    "\n",
    "\n",
    "def get_most_dominant_color(bar):\n",
    "    '''\n",
    "     This functions takes all the colors and returns the most dominant color, ratio and rgb_color\n",
    "     Input: all the colors\n",
    "     Output: Most dominant color with its ratio and rgb value\n",
    "    '''\n",
    "\n",
    "    dominant_color = \"black\"\n",
    "    max_ratio = 0\n",
    "\n",
    "    for colors in bar:\n",
    "        if colors[1] > max_ratio:\n",
    "            max_ratio = colors[1]\n",
    "            dominant_color = colors[0]\n",
    "            rgb_color = colors[2]\n",
    "        \n",
    "    return dominant_color, max_ratio, rgb_color  \n",
    "\n",
    "\n",
    "\n",
    "def get_sorted_colors(all_colors):\n",
    "    '''\n",
    "     This functions takes all the colors which can be duplicate and returns the list of unique colors in\n",
    "     decending order based on their total ratio\n",
    "     Input: all the colors\n",
    "     Output: list of unoque colors in decending order\n",
    "    '''\n",
    "    \n",
    "    color_pixels = {}\n",
    "\n",
    "    for color in all_colors:\n",
    "        if color[0] not in color_pixels:\n",
    "            color_pixels[color[0]] = color[1]\n",
    "        else:\n",
    "            color_pixels[color[0]] += color[1]\n",
    "\n",
    "    sorted_colors_pixels = sorted(color_pixels.items(), key=lambda x: x[1], reverse=True) \n",
    "    \n",
    "    return sorted_colors_pixels\n",
    "\n",
    "\n",
    "\n",
    "def get_colors(hist, centroids):\n",
    "    '''\n",
    "     This functions takes the histogram and labels and returns dominant color, it's ratio, rgb value and all colors\n",
    "     Input: histogram and labels\n",
    "     Output: dominant_color, ratio, rgb_color, all_colors\n",
    "    '''\n",
    "    \n",
    "    # initialize the bar chart representing the relative frequency of each of the colors\n",
    "    bar = np.zeros((50, 300, 3), dtype = \"uint8\")\n",
    "    startX = 0\n",
    "    all_colors = []\n",
    "    \n",
    "    # loop over the percentage and color of each cluster\n",
    "    for (percent, color) in zip(hist, centroids):\n",
    "        requested_color = color.astype(int)\n",
    "        \n",
    "        actual_name, closest_name = get_color_name(requested_color)\n",
    "        \n",
    "        if actual_name is None:\n",
    "            all_colors.append([closest_name, percent, requested_color])\n",
    "        else:\n",
    "            all_colors.append([actual_name, percent, requested_color])\n",
    "            \n",
    "        endX = startX + (percent * 300)\n",
    "        cv2.rectangle(bar, (int(startX), 0), (int(endX), 50), color.astype(\"uint8\").tolist(), -1)\n",
    "        startX = endX\n",
    "         \n",
    "    dominant_color, ratio, rgb_color = get_most_dominant_color(all_colors)\n",
    "\n",
    "    return bar, dominant_color, ratio, rgb_color, all_colors\n",
    "\n",
    "\n",
    "\n",
    "def mean_image(image, segments):\n",
    "    '''\n",
    "    This function takes segmented image and number of segments as input and returns the image with\n",
    "    average of superpixels\n",
    "    Input: Segmented Image, No of segments\n",
    "    Output: Image with average of colors in superpixels\n",
    "    '''\n",
    "\n",
    "    reshaped_image = image.reshape((image.shape[0]*image.shape[1], image.shape[2]))\n",
    "    segment_1d = np.reshape(segments, -1)    \n",
    "    unique_segment = np.unique(segment_1d)\n",
    "    img_shape = np.zeros(reshaped_image.shape)\n",
    "    \n",
    "    for i in unique_segment:\n",
    "        loc = np.where(segment_1d==i)[0]\n",
    "        img_mean = np.mean(reshaped_image[loc,:], axis=0)\n",
    "        img_shape[loc,:] = img_mean\n",
    "        \n",
    "    out_img = np.reshape(img_shape,[image.shape[0], image.shape[1], image.shape[2]]).astype('uint8')\n",
    "    \n",
    "    return out_img \n",
    "    \n",
    "\n",
    "\n",
    "def segment_image(cropped_img, numSegments):\n",
    "    '''\n",
    "    Segements the cropped image as input and no of segments and convert it into superpixels\n",
    "    Input: Cropped Image, Number of segments\n",
    "    Output: Segmented image and image with average of colors in superpixels\n",
    "    '''\n",
    "\n",
    "    converted_img = img_as_float(cropped_img[:,:,::-1]) #convert it to a floating point data type\n",
    "    segments = slic(converted_img, n_segments=numSegments, compactness=10, sigma=5, convert2lab=True)\n",
    "    \n",
    "    # Average the color in each superpixel.\n",
    "    out_img = mean_image(cropped_img, segments)\n",
    "    \n",
    "    return out_img\n",
    "\n",
    "\n",
    "\n",
    "def is_white_pixel(x):\n",
    "    x = x.tolist()\n",
    "    if x[0]==x[1]==x[2]==255:\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "\n",
    "def segment_and_cluster(num_segments, cropped_img, cluster_size):\n",
    "    '''\n",
    "    This function segments the image, cluster the pixels using kmeans clustering and return the complete details of colors\n",
    "    Input: number of segments to be done, cropped image, cluster size\n",
    "    Output: bar, dominant_color, ratio, rgb_color, all_colors\n",
    "    '''\n",
    "    \n",
    "    # Segmentation of the image into superpixels and taking average of superpixels\n",
    "    out_img = segment_image(cropped_img, num_segments)\n",
    "    \n",
    "    # Removing the white background before clustering\n",
    "    reshaped_img = out_img.reshape((-1,3))\n",
    "    mask = np.apply_along_axis(is_white_pixel, 1, reshaped_img)\n",
    "    white_removed = reshaped_img[mask]\n",
    "    \n",
    "    # cluster the pixel intensities   \n",
    "    clt = KMeans(n_clusters = cluster_size, random_state=42)\n",
    "    clt.fit(white_removed)\n",
    "    #clt.fit(filtered_img.reshape(-1,3))\n",
    "    \n",
    "\n",
    "    hist = centroid_histogram(clt)   \n",
    "    bar, dominant_color, ratio, rgb_color, all_colors = get_colors(hist, clt.cluster_centers_)\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.axis(\"off\")\n",
    "    plt.imshow(bar)\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "    return bar, dominant_color, ratio, rgb_color, all_colors\n",
    "   \n",
    "    \n",
    "    \n",
    "def calculate_luminace(color_code):\n",
    "    '''\n",
    "    This function calculates the luminance of color\n",
    "    Input: a color code in rgb\n",
    "    Output: luminance\n",
    "    '''\n",
    "    \n",
    "    index = float(color_code)/255 \n",
    "\n",
    "    if index < 0.03928:\n",
    "        return index / 12.92\n",
    "    else:\n",
    "        return ((index + 0.055) / 1.055)**2.4\n",
    "    \n",
    "\n",
    "    \n",
    "def calculate_relative_luminance(rgb):\n",
    "    '''\n",
    "    This function calculates the relative luminance of the color using the 3 channels of R, G, B\n",
    "    Input: a color in rgb\n",
    "    Output: relative luminance\n",
    "    '''\n",
    "    \n",
    "    return 0.2126*calculate_luminace(rgb[0]) + 0.7152*calculate_luminace(rgb[1]) + 0.0722*calculate_luminace(rgb[2])\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total sellers :  114\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAV0AAABGCAYAAABv7kdbAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAABJ0lEQVR4nO3asQ3CUAxAQT5irFSIsSOq7GUmAInmpchd68bVkwuvmbkB0LifvQDAlYguQEh0AUKiCxASXYCQ6AKEHr+G2/PlnwzgT8d7X99mLl2AkOgChEQXICS6ACHRBQiJLkBIdAFCogsQEl2AkOgChEQXICS6ACHRBQiJLkBIdAFCogsQEl2AkOgChEQXICS6ACHRBQiJLkBIdAFCogsQEl2AkOgChEQXICS6ACHRBQiJLkBIdAFCogsQEl2AkOgChEQXICS6ACHRBQiJLkBIdAFCogsQEl2AkOgChEQXICS6ACHRBQiJLkBIdAFCogsQEl2AkOgChEQXICS6ACHRBQiJLkBIdAFCa2bO3gHgMly6ACHRBQiJLkBIdAFCogsQEl2A0AelUwqHLk167wAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAV0AAABGCAYAAABv7kdbAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAABMUlEQVR4nO3asQ3CQBAAQT9CrsYiAlEmMRmdPhWYcC3kmfSCv2h1wY855wJA43L0AgBnIroAIdEFCIkuQEh0AUKiCxC6/hreHk//yYC/93m/lnVds/e27T72Zi5dgJDoAoREFyAkugAh0QUIiS5ASHQBQqILEBJdgJDoAoREFyAkugAh0QUIiS5ASHQBQqILEBJdgJDoAoREFyAkugAh0QUIiS5ASHQBQqILEBJdgJDoAoREFyAkugAh0QUIiS5ASHQBQqILEBJdgJDoAoREFyAkugAh0QUIiS5ASHQBQqILEBJdgJDoAoREFyAkugAh0QUIiS5ASHQBQqILEBJdgJDoAoREFyAkugAh0QUIiS5ASHQBQmPOefQOAKfh0gUIiS5ASHQBQqILEBJdgJDoAoS+owsKh9YXBiEAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAV0AAABGCAYAAABv7kdbAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAABSklEQVR4nO3asW0CQRBA0TtENU4sUYiFqMAicBnESHRA4JAiL1kqwIHFfQLeS0daTfQ1wc5jjAmAxubVCwC8E9EFCIkuQEh0AUKiCxASXYDQ9q/hx+fOfzLgX36O39Nh/7XK28uyTKfLeZW3n+F2/Z0fzVy6ACHRBQiJLkBIdAFCogsQEl2AkOgChEQXICS6ACHRBQiJLkBIdAFCogsQEl2AkOgChEQXICS6ACHRBQiJLkBIdAFCogsQEl2AkOgChEQXICS6ACHRBQiJLkBIdAFCogsQEl2AkOgChEQXICS6ACHRBQiJLkBIdAFCogsQEl2AkOgChEQXICS6ACHRBQiJLkBIdAFCogsQEl2AkOgChEQXICS6ACHRBQiJLkBIdAFCogsQEl2AkOgChOYxxqt3AHgbLl2AkOgChEQXICS6ACHRBQiJLkDoDneoEId3cedgAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAV0AAABGCAYAAABv7kdbAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAABKklEQVR4nO3asQ3CUAxAQT5iq0zECgyQFZgoc5kJQKJ5KXLXunH15MJrZm4ANO5nLwBwJaILEBJdgJDoAoREFyAkugChx6/h67n5JwP40/4+1reZSxcgJLoAIdEFCIkuQEh0AUKiCxASXYCQ6AKERBcgJLoAIdEFCIkuQEh0AUKiCxASXYCQ6AKERBcgJLoAIdEFCIkuQEh0AUKiCxASXYCQ6AKERBcgJLoAIdEFCIkuQEh0AUKiCxASXYCQ6AKERBcgJLoAIdEFCIkuQEh0AUKiCxASXYCQ6AKERBcgJLoAIdEFCIkuQEh0AUKiCxASXYCQ6AKERBcgJLoAIdEFCIkuQEh0AUKiCxASXYDQmpmzdwC4DJcuQEh0AUKiCxASXYCQ6AKERBcg9AGHAQqHoWConAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Color result dict :  {1: [('teal', 1.0)], 2: [('black', 0.9171119089081161), ('silver', 0.08288809109188384)], 3: [('black', 0.816510746295227), ('gray', 0.11596458755086717), ('white', 0.06752466615390584)]}\n",
      "k2_dominant_ratio :  0.9171119089081161\n",
      "Total colors are :  1\n",
      "Entropy of handbag is :  0\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAV0AAABGCAYAAABv7kdbAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAABKUlEQVR4nO3asQ3CUAxAQT5iwHSMwSwZgy4bmglAonkpcte6cfXkwmtmbgA07mcvAHAlogsQEl2AkOgChEQXICS6AKHHr+H2fPknA/jT8d7Xt5lLFyAkugAh0QUIiS5ASHQBQqILEBJdgJDoAoREFyAkugAh0QUIiS5ASHQBQqILEBJdgJDoAoREFyAkugAh0QUIiS5ASHQBQqILEBJdgJDoAoREFyAkugAh0QUIiS5ASHQBQqILEBJdgJDoAoREFyAkugAh0QUIiS5ASHQBQqILEBJdgJDoAoREFyAkugAh0QUIiS5ASHQBQqILEBJdgJDoAoREFyAkugAh0QUIiS5ASHQBQqILEBJdgNCambN3ALgMly5ASHQBQqILEBJdgJDoAoREFyD0AV0hCoeCFDonAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAV0AAABGCAYAAABv7kdbAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAABN0lEQVR4nO3asQ3CUAxAwQTRA6MgUSHYgwkYgiqbfyaACl6K3LVuXD258DzGmABo7NZeAGBLRBcgJLoAIdEFCIkuQEh0AUL7b8PL7eGfDPi7+/U8La/n2mv8zOF4mj/NXLoAIdEFCIkuQEh0AUKiCxASXYCQ6AKERBcgJLoAIdEFCIkuQEh0AUKiCxASXYCQ6AKERBcgJLoAIdEFCIkuQEh0AUKiCxASXYCQ6AKERBcgJLoAIdEFCIkuQEh0AUKiCxASXYCQ6AKERBcgJLoAIdEFCIkuQEh0AUKiCxASXYCQ6AKERBcgJLoAIdEFCIkuQEh0AUKiCxASXYCQ6AKERBcgJLoAIdEFCIkuQEh0AUKiCxASXYCQ6AKE5jHG2jsAbIZLFyAkugAh0QUIiS5ASHQBQqILEHoDZdkKh1IRs84AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAV0AAABGCAYAAABv7kdbAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAABS0lEQVR4nO3asUkEURhG0Rmxlwm2H9F0C1CYOkytxQYsRkws4W0FLizCfSxzTvqSL7r8wVvHGAsAjYfZAwCORHQBQqILEBJdgJDoAoREFyD0eO1x207+k92hl+enZd/fZs/4t++f3+X8+jF7Btzs6/N9/evNpQsQEl2AkOgChEQXICS6ACHRBQiJLkBIdAFCogsQEl2AkOgChEQXICS6ACHRBQiJLkBIdAFCogsQEl2AkOgChEQXICS6ACHRBQiJLkBIdAFCogsQEl2AkOgChEQXICS6ACHRBQiJLkBIdAFCogsQEl2AkOgChEQXICS6ACHRBQiJLkBIdAFCogsQEl2AkOgChEQXICS6ACHRBQiJLkBIdAFCogsQEl2AkOgChEQXICS6ACHRBQiJLkBoHWPM3gBwGC5dgJDoAoREFyAkugAh0QUIiS5A6ALWthCHmnhEGAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Color result dict :  {1: [('gray', 1.0)], 2: [('navy', 0.84867190672137), ('white', 0.15132809327862992)], 3: [('teal', 0.5192259909945398), ('black', 0.3310494451885026), ('white', 0.14972456381695765)]}\n",
      "k2_dominant_ratio :  0.84867190672137\n",
      "Total colors are :  2\n",
      "Entropy of handbag is :  0.6131539036385149\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAV0AAABGCAYAAABv7kdbAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAABKUlEQVR4nO3asQ3CUAxAQT5itXQswkRZhC7DmQlAonkpcte6cfXkwmtmbgA07mcvAHAlogsQEl2AkOgChEQXICS6AKHHr+H7tfknA/jTcz/Wt5lLFyAkugAh0QUIiS5ASHQBQqILEBJdgJDoAoREFyAkugAh0QUIiS5ASHQBQqILEBJdgJDoAoREFyAkugAh0QUIiS5ASHQBQqILEBJdgJDoAoREFyAkugAh0QUIiS5ASHQBQqILEBJdgJDoAoREFyAkugAh0QUIiS5ASHQBQqILEBJdgJDoAoREFyAkugAh0QUIiS5ASHQBQqILEBJdgJDoAoREFyAkugAh0QUIiS5ASHQBQqILEBJdgNCambN3ALgMly5ASHQBQqILEBJdgJDoAoREFyD0AesACodS5TH6AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAV0AAABGCAYAAABv7kdbAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAABOklEQVR4nO3asQ3CUAxAwQSxISsxBgMwBQtQIJagRTRpPxOAqF6Qcte6cfXkwvMYYwKgsVt7AYAtEV2AkOgChEQXICS6ACHRBQjtvw2X1/Mv/ske18t0Px3XXgPgJ4fzbf40c+kChEQXICS6ACHRBQiJLkBIdAFCogsQEl2AkOgChEQXICS6ACHRBQiJLkBIdAFCogsQEl2AkOgChEQXICS6ACHRBQiJLkBIdAFCogsQEl2AkOgChEQXICS6ACHRBQiJLkBIdAFCogsQEl2AkOgChEQXICS6ACHRBQiJLkBIdAFCogsQEl2AkOgChEQXICS6ACHRBQiJLkBIdAFCogsQEl2AkOgChEQXICS6ACHRBQiJLkBIdAFCogsQmscYa+8AsBkuXYCQ6AKERBcgJLoAIdEFCIkuQOgNMKAQh6LMwQwAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAV0AAABGCAYAAABv7kdbAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAABR0lEQVR4nO3aoXGCQRhF0X8z6QPBpBXqSRepgVIwGCiEIphM3KYCEIi7gnPsmqfufGLHnHMDoPGxegDAOxFdgJDoAoREFyAkugAh0QUIfT57/Pu9L/1Pdv753m7X08oJvGCMse2/dqtnwDKH42U8enPpAoREFyAkugAh0QUIiS5ASHQBQqILEBJdgJDoAoREFyAkugAh0QUIiS5ASHQBQqILEBJdgJDoAoREFyAkugAh0QUIiS5ASHQBQqILEBJdgJDoAoREFyAkugAh0QUIiS5ASHQBQqILEBJdgJDoAoREFyAkugAh0QUIiS5ASHQBQqILEBJdgJDoAoREFyAkugAh0QUIiS5ASHQBQqILEBJdgJDoAoREFyAkugAh0QUIiS5ASHQBQqILEBpzztUbAN6GSxcgJLoAIdEFCIkuQEh0AUKiCxD6B12EEIfIfihyAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Color result dict :  {1: [('gray', 1.0)], 2: [('olive', 0.8950738150738151), ('white', 0.10492618492618493)], 3: [('olive', 0.9007595182595183), ('white', 0.09924048174048174)]}\n",
      "k2_dominant_ratio :  0.8950738150738151\n",
      "Total colors are :  2\n",
      "Entropy of handbag is :  0.4844194987198679\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAV0AAABGCAYAAABv7kdbAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAABJElEQVR4nO3asQ2DUAxAwfyIucjsYTFngiDRPAruWjeunlx4zcwLgMb77gUAnkR0AUKiCxASXYCQ6AKERBcgtJ0NP/vunwzgou9xrH8zly5ASHQBQqILEBJdgJDoAoREFyAkugAh0QUIiS5ASHQBQqILEBJdgJDoAoREFyAkugAh0QUIiS5ASHQBQqILEBJdgJDoAoREFyAkugAh0QUIiS5ASHQBQqILEBJdgJDoAoREFyAkugAh0QUIiS5ASHQBQqILEBJdgJDoAoREFyAkugAh0QUIiS5ASHQBQqILEBJdgJDoAoREFyAkugAh0QUIiS5ASHQBQqILEBJdgJDoAoREFyAkugChNTN37wDwGC5dgJDoAoREFyAkugAh0QUIiS5A6AeArwqHzfp7eQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAV0AAABGCAYAAABv7kdbAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAABL0lEQVR4nO3asQ3DMAwAQSvwAN7C2X+GeC5lgrh8BfBdy4bVgwXHnHMDoPFavQDAk4guQEh0AUKiCxASXYCQ6AKE9rvh+zz9kwF/63Nd277fZmyJ4zjGr5lLFyAkugAh0QUIiS5ASHQBQqILEBJdgJDoAoREFyAkugAh0QUIiS5ASHQBQqILEBJdgJDoAoREFyAkugAh0QUIiS5ASHQBQqILEBJdgJDoAoREFyAkugAh0QUIiS5ASHQBQqILEBJdgJDoAoREFyAkugAh0QUIiS5ASHQBQqILEBJdgJDoAoREFyAkugAh0QUIiS5ASHQBQqILEBJdgJDoAoREFyAkugAh0QUIiS5ASHQBQqILEBJdgNCYc67eAeAxXLoAIdEFCIkuQEh0AUKiCxASXYDQF3zDCofO9IqhAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAV0AAABGCAYAAABv7kdbAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAABQ0lEQVR4nO3aoQ0CQRBA0TtCCVcCFIlCUgIaRxXQDoQilgpAkX+C9+wkm1E/I3YeY0wANDZrLwDwT0QXICS6ACHRBQiJLkBIdAFC22/D/W7vPxnwc7f7bVqWZe01ptfzMZ2Oh5+/e75c508zly5ASHQBQqILEBJdgJDoAoREFyAkugAh0QUIiS5ASHQBQqILEBJdgJDoAoREFyAkugAh0QUIiS5ASHQBQqILEBJdgJDoAoREFyAkugAh0QUIiS5ASHQBQqILEBJdgJDoAoREFyAkugAh0QUIiS5ASHQBQqILEBJdgJDoAoREFyAkugAh0QUIiS5ASHQBQqILEBJdgJDoAoREFyAkugAh0QUIiS5ASHQBQqILEBJdgJDoAoREFyAkugCheYyx9g4Af8OlCxASXYCQ6AKERBcgJLoAIdEFCL0BoD4Qh6sK0t8AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Color result dict :  {1: [('black', 1.0)], 2: [('black', 0.8870406740862471), ('white', 0.11295932591375295)], 3: [('black', 0.8363105179009624), ('white', 0.10358798615842986), ('gray', 0.06010149594060773)]}\n",
      "k2_dominant_ratio :  0.8870406740862471\n",
      "Total colors are :  2\n",
      "Entropy of handbag is :  0.5087781507068535\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-5884aa51275f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    145\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[0mclient\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvision\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mImageAnnotatorClient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcredentials\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m \u001b[0mproduct_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread_images\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Group4/Sellers/'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclient\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#seller_105548/Images')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    148\u001b[0m \u001b[0mproduct_info\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-19-5884aa51275f>\u001b[0m in \u001b[0;36mread_images\u001b[0;34m(dir_path, client)\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 142\u001b[0;31m     \u001b[0mitem_details\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnew_cols\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mitem_details\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Image_path'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocess_images\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mitem_details\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/pytorch/lib/python3.7/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, func, convert_dtype, args, **kwds)\u001b[0m\n\u001b[1;32m   4198\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4199\u001b[0m                 \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4200\u001b[0;31m                 \u001b[0mmapped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_infer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconvert_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4201\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4202\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSeries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/lib.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m<ipython-input-19-5884aa51275f>\u001b[0m in \u001b[0;36mprocess_images\u001b[0;34m(img_path)\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcluster_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0;31m# print(\"Processing for bag\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m         \u001b[0mbar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbag_dominant_color\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbag_ratio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbag_rgb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_bag_colors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msegment_and_cluster\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_segments\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcropped_bag\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m         \u001b[0msorted_bag_colors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_sorted_colors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_bag_colors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0mcolor_result\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msorted_bag_colors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-c318fda420d7>\u001b[0m in \u001b[0;36msegment_and_cluster\u001b[0;34m(num_segments, cropped_img, cluster_size)\u001b[0m\n\u001b[1;32m    266\u001b[0m     \u001b[0;31m# Removing the white background before clustering\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m     \u001b[0mreshaped_img\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mout_img\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 268\u001b[0;31m     \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_along_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_white_pixel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreshaped_img\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    269\u001b[0m     \u001b[0mwhite_removed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreshaped_img\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    270\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mapply_along_axis\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/pytorch/lib/python3.7/site-packages/numpy/lib/shape_base.py\u001b[0m in \u001b[0;36mapply_along_axis\u001b[0;34m(func1d, axis, arr, *args, **kwargs)\u001b[0m\n\u001b[1;32m    400\u001b[0m     \u001b[0mbuff\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mind0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    401\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mind\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minds\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 402\u001b[0;31m         \u001b[0mbuff\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mind\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0masanyarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc1d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minarr_view\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mind\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    403\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    404\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmatrix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/pytorch/lib/python3.7/site-packages/numpy/core/_asarray.py\u001b[0m in \u001b[0;36masanyarray\u001b[0;34m(a, dtype, order)\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m     \"\"\"\n\u001b[0;32m--> 136\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubok\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    137\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def process_images(img_path):\n",
    "    \n",
    "    if not os.path.isfile(img_path):\n",
    "        return pd.Series([None]*5)\n",
    "    \n",
    "    # Get the cropped bag\n",
    "    img = Img.open(img_path)\n",
    "    cropped_bag = crop_image(img, img_path) \n",
    "    height, width = cropped_bag.size\n",
    "    bag_size = height*width\n",
    "    #print(\"Bag size : \", bag_size)\n",
    "    #Img.fromarray(np.array(cropped_bag)).show()\n",
    "    \n",
    "    # Get the cropped logo\n",
    "    cropped_img_path = 'cropped.jpg'\n",
    "    cropped_img = Img.open(cropped_img_path)\n",
    "    cropped_logo= get_logo(cropped_img, cropped_img_path) \n",
    "    \n",
    "\n",
    "    # converting cropped bag img from PIL to numpy array\n",
    "    cropped_bag = np.array(cropped_bag)\n",
    "    \n",
    "\n",
    "    # Segmentation of the image into superpixels, taking average of superpixels and then doing clustering\n",
    "    num_segments = 600\n",
    "    cluster_size = 4\n",
    "\n",
    "    color_result = {}\n",
    "    rgb_colors = []\n",
    "    for i in range(1, cluster_size):\n",
    "        # print(\"Processing for bag\")\n",
    "        bar, bag_dominant_color, bag_ratio, bag_rgb, all_bag_colors = segment_and_cluster(num_segments, cropped_bag, i)\n",
    "        sorted_bag_colors = get_sorted_colors(all_bag_colors)\n",
    "        color_result[i] = sorted_bag_colors\n",
    "        rgb_colors.append(bag_rgb)\n",
    "\n",
    "\n",
    "    #print(\"Processing for logo\")\n",
    "    \n",
    "    if cropped_logo is not None:\n",
    "        cropped_logo = np.array(cropped_logo)  # converting pil image to numpy array\n",
    "        num_segments = 300\n",
    "        cluster_size = 1\n",
    "\n",
    "        # Get logo size \n",
    "        height, width, _ = cropped_logo.shape\n",
    "        logo_size = height * width\n",
    "        #print(\"Logo size : \", logo_size)        \n",
    "        \n",
    "        bar, logo_dominant_color, logo_ratio, logo_rgb, all_logo_colors = segment_and_cluster(num_segments, cropped_logo, cluster_size)\n",
    "        sorted_logo_colors = get_sorted_colors(all_logo_colors)\n",
    "        #print(\"logo colors : \", sorted_logo_colors)\n",
    "\n",
    "        # Calculate contrast ratio of logo to bag\n",
    "        contrast_ratio = (calculate_relative_luminance(logo_rgb)+0.05) / (calculate_relative_luminance(bag_rgb)+0.05)\n",
    "        #print(\"Contrast ratio is : \", contrast_ratio)\n",
    "        \n",
    "        # Calculate Logo Conspicuousness\n",
    "        relative_size = logo_size/bag_size\n",
    "        logo_conspicuousness = contrast_ratio * relative_size\n",
    "        #print(\"Logo Conspicuousness : \", logo_conspicuousness)\n",
    "\n",
    "    else:\n",
    "        logo_size = 0.0\n",
    "        contrast_ratio = 0.0\n",
    "        logo_conspicuousness = 0.0\n",
    "        \n",
    "        \n",
    "\n",
    "    # Find total number of colors and entropy of the bag\n",
    "\n",
    "    print(\"Color result dict : \", color_result)\n",
    "    k2 = color_result[2]\n",
    "    k3 = color_result[3]\n",
    "    k2_dominant_ratio = k2[0][1]\n",
    "    print(\"k2_dominant_ratio : \", k2_dominant_ratio)\n",
    "    if k2_dominant_ratio >= 0.9:\n",
    "        total_colors = 1\n",
    "        entropy = 0\n",
    "    elif k2_dominant_ratio < 0.9 and k2_dominant_ratio >= 0.6:\n",
    "        total_colors = 2\n",
    "        entropy = -1 * ((k2_dominant_ratio * log(k2_dominant_ratio, 2)) + (k2[1][1] * log(k2[1][1], 2)))\n",
    "    else:\n",
    "        total_colors = 3\n",
    "        entropy = -1 * ((k3[0][1] * log(k3[0][1], 2) + (k3[1][1] * log(k3[1][1], 2)) +\n",
    "                       (k3[2][1] * log(k3[2][1], 2))))\n",
    "                \n",
    "    print(\"Total colors are : \", total_colors)\n",
    "    print(\"Entropy of handbag is : \", entropy)\n",
    "    \n",
    "    return_output = (logo_size,\n",
    "                     round(contrast_ratio, 3),\n",
    "                     round(logo_conspicuousness, 3),\n",
    "                     total_colors, round(entropy, 3))\n",
    "\n",
    "    for i in range(1, 4):\n",
    "        colors, ratios = list(zip(*color_result[i]))\n",
    "        ratios = [round(val, 2) for val in ratios]\n",
    "        return_output = return_output + (colors, ratios)\n",
    "\n",
    "    return pd.Series(return_output)\n",
    "\n",
    "\n",
    "def read_images(dir_path, client):\n",
    "    count = 0\n",
    "    \n",
    "    item_details = pd.DataFrame(columns = ['Seller', 'Item ID', 'category', 'brand', 'color', 'Image_path']) \n",
    "    dir_names = os.listdir(dir_path)\n",
    "    print(\"Total sellers : \", len(dir_names))\n",
    "    for seller in dir_names:\n",
    "        if not seller.startswith('seller_'):\n",
    "            continue\n",
    "        \n",
    "        product_info = pd.read_csv(dir_path+seller+'/ProductInfo.csv', usecols=[\"Item ID\", \"category\", \"brand\", \"color\"])\n",
    "        product_info['Seller'] = seller\n",
    "\n",
    "        \n",
    "        # Get the image names and add to the dataframe\n",
    "        product_info['Image_path'] = ''\n",
    "        img_fnames = os.listdir(dir_path + seller + '/Images')\n",
    "        for idx in range(len(img_fnames)):\n",
    "            img_path = dir_path + seller + '/Images/' + img_fnames[idx]\n",
    "            img_id = int(img_fnames[idx].split('-')[0])\n",
    "            img_fnames[idx] =  [img_path, img_id]\n",
    "        \n",
    "        img_ids, img_names = map(list, zip(*img_fnames))\n",
    "        product_info['Image_path'] = product_info['Item ID'].replace(to_replace=img_names, value=img_ids)\n",
    "        \n",
    "        item_details = pd.concat([item_details, product_info], ignore_index=True)\n",
    "        \n",
    "    \n",
    "    # Removing the rows that doesn't contain bags\n",
    "    item_details = item_details[item_details['category'].str.match('Bags')]\n",
    "    \n",
    "    #import ipdb; ipdb.set_trace()\n",
    "    \n",
    "    new_cols = ['Logo Size', 'Logo Contrast', 'Logo Conspicuousness', 'Number of colors', 'Color Entropy']\n",
    "    for col in new_cols:\n",
    "        item_details[col] = None\n",
    "    \n",
    "        \n",
    "    item_details[new_cols] = item_details['Image_path'].apply(process_images)\n",
    "    \n",
    "    return item_details\n",
    "\n",
    "client = vision.ImageAnnotatorClient(credentials=creds)\n",
    "product_info = read_images('Group4/Sellers/', client) #seller_105548/Images')\n",
    "product_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "product_info.to_csv(\"bag_details.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting unique handbag brands:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "bags = pd.read_csv(\"bag_details_v1.csv\")\n",
    "bag_brands = bags['brand']\n",
    "bag_brands.value_counts().to_csv(\"bag_brands.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
